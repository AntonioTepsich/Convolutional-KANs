{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import KANConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_binary(mnist_data, binary=False):\n",
    "    \"\"\"\n",
    "    Just keep the 0 and 1 classes\n",
    "    \"\"\"\n",
    "    if binary:\n",
    "        mnist_data.data = mnist_data.data[(mnist_data.targets == 0) | (mnist_data.targets == 1)]\n",
    "        mnist_data.targets = mnist_data.targets[(mnist_data.targets == 0) | (mnist_data.targets == 1)]\n",
    "    return mnist_data\n",
    "\n",
    "\n",
    "\n",
    "# Transformaciones\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Cargar MNIST y filtrar por dos clases\n",
    "all_mnist_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# mnist_train = data_to_binary(all_mnist_train, binary=True)\n",
    "mnist_train = data_to_binary(all_mnist_train, binary=False)\n",
    "\n",
    "all_mnist_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# mnist_test = data_to_binary(all_mnist_test, binary=True)\n",
    "mnist_test = data_to_binary(all_mnist_test, binary=False)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, criterion):\n",
    "    # Set the model to training mode\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    # Switch the model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (target == predicted).sum().item()\n",
    "\n",
    "            # Collect all targets and predictions for metric calculations\n",
    "            all_targets.extend(target.view_as(predicted).cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    precision = precision_score(all_targets, all_predictions, average='macro')\n",
    "    recall = recall_score(all_targets, all_predictions, average='macro')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "    # Normalize test loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy, precision, recall, f1))\n",
    "\n",
    "    return test_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim 2\n",
      "in feat 4\n",
      "shape Flatten(start_dim=1, end_dim=-1)\n",
      "dim 2\n",
      "in feat 512\n",
      "Training on cuda\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagen torch.Size([64, 1, 28, 28])\n",
      "matrix torch.Size([64, 28, 28])\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub tensor([-1., -1., -1., -1.], device='cuda:0')\n",
      "tensor([[-1., -1., -1., -1.]], device='cuda:0') tensor([[-0.2689, -0.2689, -0.2689, -0.2689]], device='cuda:0') Parameter containing:\n",
      "tensor([[-0.3974, -0.0058, -0.3487, -0.1954]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base out tensor([[0.2547]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "spline out tensor([[0.0016]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "matrix out 0.25637078285217285\n",
      "No gradient for base_weight\n",
      "No gradient for spline_weight\n",
      "No gradient for spline_scaler\n",
      "sub "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining on\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m         test_loss, accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m test(model, device, test_loader, criterion)\n\u001b[0;32m     25\u001b[0m         epoch_nums\u001b[38;5;241m.\u001b[39mappend(epoch)\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch, criterion)\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Push the data forward through the model layers\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Get the loss\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\ckan\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\ckan\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MSI\\Desktop\\Proyectos\\ckan\\kan_convolutional\\KANConv.py:112\u001b[0m, in \u001b[0;36mKAN_Convolutional_Network.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# input 3x32x32, output 32x32x32\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# input 32x32x32, output 32x32x32\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# input 32x32x32, output 32x16x16\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(x)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\ckan\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\ckan\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MSI\\Desktop\\Proyectos\\ckan\\kan_convolutional\\KANConv.py:63\u001b[0m, in \u001b[0;36mKAN_Convolution_Linears.forward\u001b[1;34m(self, x, update_grid)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, update_grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_filter_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MSI\\Desktop\\Proyectos\\ckan\\kan_convolutional\\convolution.py:120\u001b[0m, in \u001b[0;36mapply_filter_to_image\u001b[1;34m(image, kernel, kernel_side, padding, rgb)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rgb:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdstack([kan_conv2d(image[:, :, z], kernel, padding\u001b[38;5;241m=\u001b[39mpadding) \n\u001b[0;32m    119\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m)])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkan_conv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkernel_side\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MSI\\Desktop\\Proyectos\\ckan\\kan_convolutional\\convolution.py:89\u001b[0m, in \u001b[0;36mkan_conv2d\u001b[1;34m(matrix, kernel, kernel_side, stride, dilation, padding)\u001b[0m\n\u001b[0;32m     87\u001b[0m submatrix \u001b[38;5;241m=\u001b[39m submatrix\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# print(\"sub\",submatrix)\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msub\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msubmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m matrix_out[i][j] \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mforward(submatrix)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# matrix_out[i][j] = kernel.forward(submatrix.flatten())\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\ckan\\lib\\site-packages\\torch\\_tensor.py:464\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    461\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[0;32m    462\u001b[0m     )\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\ckan\\lib\\site-packages\\torch\\_tensor_str.py:697\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[0;32m    696\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\ckan\\lib\\site-packages\\torch\\_tensor_str.py:617\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    615\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    616\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    620\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\ckan\\lib\\site-packages\\torch\\_tensor_str.py:349\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[0;32m    347\u001b[0m     )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\ckan\\lib\\site-packages\\torch\\_tensor_str.py:137\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = KANConv.KAN_Convolutional_Network()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 10\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch, criterion)\n",
    "        test_loss, accuracy, precision, recall, f1 = test(model, device, test_loader, criterion)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "        scheduler.step()\n",
    "        print('')\n",
    "        print(\"lr: \", optimizer.param_groups[0]['lr'])\n",
    "        print(\"test loss: \", test_loss)\n",
    "        print(\"accuracy: \", accuracy)\n",
    "        print(\"precision: \", precision)\n",
    "        print(\"recall: \", recall)\n",
    "        print(\"f1: \", f1)\n",
    "        print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
