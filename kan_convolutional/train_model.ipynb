{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex_\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\alex_\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import KAN_Network\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_binary(mnist_data, binary=False):\n",
    "    \"\"\"\n",
    "    Just keep the 0 and 1 classes\n",
    "    \"\"\"\n",
    "    if binary:\n",
    "        mnist_data.data = mnist_data.data[(mnist_data.targets == 0) | (mnist_data.targets == 1)]\n",
    "        mnist_data.targets = mnist_data.targets[(mnist_data.targets == 0) | (mnist_data.targets == 1)]\n",
    "    return mnist_data\n",
    "\n",
    "\n",
    "\n",
    "# Transformaciones\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Cargar MNIST y filtrar por dos clases\n",
    "all_mnist_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# mnist_train = data_to_binary(all_mnist_train, binary=True)\n",
    "mnist_train = data_to_binary(all_mnist_train, binary=False)\n",
    "\n",
    "all_mnist_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# mnist_test = data_to_binary(all_mnist_test, binary=True)\n",
    "mnist_test = data_to_binary(all_mnist_test, binary=False)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, criterion):\n",
    "    # Set the model to training mode\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    # Switch the model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (target == predicted).sum().item()\n",
    "\n",
    "            # Collect all targets and predictions for metric calculations\n",
    "            all_targets.extend(target.view_as(predicted).cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    precision = precision_score(all_targets, all_predictions, average='macro')\n",
    "    recall = recall_score(all_targets, all_predictions, average='macro')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "    # Normalize test loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy, precision, recall, f1))\n",
    "\n",
    "    return test_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:06<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.585755\n",
      "\n",
      "Test set: Average loss: 0.0031, Accuracy: 9435/10000 (94%), Precision: 0.94, Recall: 0.94, F1 Score: 0.94\n",
      "\n",
      "\n",
      "lr:  0.0008\n",
      "test loss:  0.0031308427271433173\n",
      "accuracy:  94.35\n",
      "precision:  0.9433808509961249\n",
      "recall:  0.9428718714870051\n",
      "f1:  0.9429771172685969\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:05<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.179099\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 9568/10000 (96%), Precision: 0.96, Recall: 0.96, F1 Score: 0.96\n",
      "\n",
      "\n",
      "lr:  0.00064\n",
      "test loss:  0.002337721869070083\n",
      "accuracy:  95.68\n",
      "precision:  0.9566476702949476\n",
      "recall:  0.9564495189642488\n",
      "f1:  0.9564911733682828\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:09<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.143203\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 9595/10000 (96%), Precision: 0.96, Recall: 0.96, F1 Score: 0.96\n",
      "\n",
      "\n",
      "lr:  0.0005120000000000001\n",
      "test loss:  0.0020276573593262584\n",
      "accuracy:  95.95\n",
      "precision:  0.9593925501634191\n",
      "recall:  0.9591425528647894\n",
      "f1:  0.9592241828854116\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:47<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.125726\n",
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 9626/10000 (96%), Precision: 0.96, Recall: 0.96, F1 Score: 0.96\n",
      "\n",
      "\n",
      "lr:  0.0004096000000000001\n",
      "test loss:  0.001865648494847119\n",
      "accuracy:  96.26\n",
      "precision:  0.9625611614849154\n",
      "recall:  0.9622054490938836\n",
      "f1:  0.9623411764910488\n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:13<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.115368\n",
      "\n",
      "Test set: Average loss: 0.0018, Accuracy: 9644/10000 (96%), Precision: 0.96, Recall: 0.96, F1 Score: 0.96\n",
      "\n",
      "\n",
      "lr:  0.0003276800000000001\n",
      "test loss:  0.0017657364635495468\n",
      "accuracy:  96.44\n",
      "precision:  0.9642511276984018\n",
      "recall:  0.9641821163827258\n",
      "f1:  0.9641873382573849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = KAN_Network.KAN_Convolutional_Network(device = device)\n",
    "model.to(device)\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 5\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch, criterion)\n",
    "        test_loss, accuracy, precision, recall, f1 = test(model, device, test_loader, criterion)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "        scheduler.step()\n",
    "        print('')\n",
    "        print(\"lr: \", optimizer.param_groups[0]['lr'])\n",
    "        print(\"test loss: \", test_loss)\n",
    "        print(\"accuracy: \", accuracy)\n",
    "        print(\"precision: \", precision)\n",
    "        print(\"recall: \", recall)\n",
    "        print(\"f1: \", f1)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39msqueeze(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre convolucion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m ax[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPost convolucion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alex_\\Documents\\Udesa\\Research\\ckan\\kan_convolutional\\KANConv.py:56\u001b[0m, in \u001b[0;36mKAN_Convolutional_Layer.forward\u001b[1;34m(self, x, update_grid)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, update_grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m#disaplce\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_convs\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiple_convs_kan_conv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mforward(x)\n",
      "File \u001b[1;32mc:\\Users\\alex_\\Documents\\Udesa\\Research\\ckan\\kan_convolutional\\convolution.py:79\u001b[0m, in \u001b[0;36mmultiple_convs_kan_conv2d\u001b[1;34m(matrix, kernels, kernel_side, stride, dilation, padding, device)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_channels):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m#for k in range(batch_size):\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m kern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_convs):\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;66;03m#print(conv_groups[:,channel,:,:].shape)\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m         matrix_out[:,kern  \u001b[38;5;241m+\u001b[39m channel\u001b[38;5;241m*\u001b[39mn_convs,:,:] \u001b[38;5;241m=\u001b[39m \u001b[43mkernels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkern\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape((batch_size,h_out,w_out))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matrix_out\n",
      "File \u001b[1;32mc:\\Users\\alex_\\Documents\\Udesa\\Research\\ckan\\kan_convolutional\\KANLinear.py:156\u001b[0m, in \u001b[0;36mKANLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features\n\u001b[1;32m--> 156\u001b[0m     base_output \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     spline_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_splines(x)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaled_spline_weight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base_output \u001b[38;5;241m+\u001b[39m spline_output\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, label = all_mnist_test[0]\n",
    "fig, ax  = plt.subplots(nrows = 1,ncols = 2)\n",
    "\n",
    "#figure.add_subplot(rows, cols, i)\n",
    "#ax.title([label])\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax[0].imshow(img.squeeze(), cmap=\"gray\")\n",
    "ax[0].set_title(\"Pre convolucion\")\n",
    "ax[1].imshow(model.conv1.forward(img.unsqueeze(0)).to(device), cmap=\"gray\")\n",
    "ax[0].set_title(\"Post convolucion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
