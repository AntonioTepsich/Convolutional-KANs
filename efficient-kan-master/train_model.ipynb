{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import convolution_kan\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:04<00:00, 2341950.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 164768.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1125673.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1088352.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def data_to_binary(mnist_data, binary=False):\n",
    "    \"\"\"\n",
    "    Just keep the 0 and 1 classes\n",
    "    \"\"\"\n",
    "    if binary:\n",
    "        mnist_data.data = mnist_data.data[(mnist_data.targets == 0) | (mnist_data.targets == 1)]\n",
    "        mnist_data.targets = mnist_data.targets[(mnist_data.targets == 0) | (mnist_data.targets == 1)]\n",
    "    return mnist_data\n",
    "\n",
    "\n",
    "\n",
    "# Transformaciones\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Cargar MNIST y filtrar por dos clases\n",
    "all_mnist_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# mnist_train = data_to_binary(all_mnist_train, binary=True)\n",
    "mnist_train = data_to_binary(all_mnist_train, binary=False)\n",
    "\n",
    "all_mnist_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# mnist_test = data_to_binary(all_mnist_test, binary=True)\n",
    "mnist_test = data_to_binary(all_mnist_test, binary=False)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, criterion):\n",
    "    # Set the model to training mode\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    # Switch the model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (target == predicted).sum().item()\n",
    "\n",
    "            # Collect all targets and predictions for metric calculations\n",
    "            all_targets.extend(target.view_as(predicted).cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    precision = precision_score(all_targets, all_predictions, average='macro')\n",
    "    recall = recall_score(all_targets, all_predictions, average='macro')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "    # Normalize test loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy, precision, recall, f1))\n",
    "\n",
    "    return test_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.189711\n",
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 9828/10000 (98%), Precision: 0.98, Recall: 0.98, F1 Score: 0.98\n",
      "\n",
      "0.0008\n",
      "\n",
      "test loss:  0.0008964011472126003\n",
      "accuracy:  98.28\n",
      "precision:  0.9829414455822884\n",
      "recall:  0.9828111975147109\n",
      "f1:  0.9828097059883113\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.052503\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 9833/10000 (98%), Precision: 0.98, Recall: 0.98, F1 Score: 0.98\n",
      "\n",
      "0.00064\n",
      "\n",
      "test loss:  0.0007634898854827043\n",
      "accuracy:  98.33\n",
      "precision:  0.983500784038584\n",
      "recall:  0.9831711178594276\n",
      "f1:  0.983259410196518\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 42.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.036364\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 9863/10000 (99%), Precision: 0.99, Recall: 0.99, F1 Score: 0.99\n",
      "\n",
      "0.0005120000000000001\n",
      "\n",
      "test loss:  0.0006414072981890058\n",
      "accuracy:  98.63\n",
      "precision:  0.9864387269455748\n",
      "recall:  0.9860939503410225\n",
      "f1:  0.9862037974155525\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 44.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.028853\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 9864/10000 (99%), Precision: 0.99, Recall: 0.99, F1 Score: 0.99\n",
      "\n",
      "0.0004096000000000001\n",
      "\n",
      "test loss:  0.0005961154789940338\n",
      "accuracy:  98.64\n",
      "precision:  0.9865372288985071\n",
      "recall:  0.9863145687225611\n",
      "f1:  0.9863758921560375\n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:20<00:00, 46.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.022136\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 9885/10000 (99%), Precision: 0.99, Recall: 0.99, F1 Score: 0.99\n",
      "\n",
      "0.0003276800000000001\n",
      "\n",
      "test loss:  0.000535092187745613\n",
      "accuracy:  98.85\n",
      "precision:  0.9885502918566502\n",
      "recall:  0.9883177898636661\n",
      "f1:  0.9884085188409906\n",
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:20<00:00, 46.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.017281\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 9882/10000 (99%), Precision: 0.99, Recall: 0.99, F1 Score: 0.99\n",
      "\n",
      "0.0002621440000000001\n",
      "\n",
      "test loss:  0.0005533403412839107\n",
      "accuracy:  98.82\n",
      "precision:  0.9882341866156341\n",
      "recall:  0.9879748901358913\n",
      "f1:  0.9880831568938216\n",
      "\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 44.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.014065\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 9875/10000 (99%), Precision: 0.99, Recall: 0.99, F1 Score: 0.99\n",
      "\n",
      "0.00020971520000000012\n",
      "\n",
      "test loss:  0.0005969544560754002\n",
      "accuracy:  98.75\n",
      "precision:  0.9875609517632575\n",
      "recall:  0.9873672779812905\n",
      "f1:  0.9873923674249194\n",
      "\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.011720\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 9891/10000 (99%), Precision: 0.99, Recall: 0.99, F1 Score: 0.99\n",
      "\n",
      "0.0001677721600000001\n",
      "\n",
      "test loss:  0.0004944570142113661\n",
      "accuracy:  98.91\n",
      "precision:  0.9890585239513726\n",
      "recall:  0.9889400124572802\n",
      "f1:  0.9889889771953084\n",
      "\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.009827\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 9884/10000 (99%), Precision: 0.99, Recall: 0.99, F1 Score: 0.99\n",
      "\n",
      "0.00013421772800000008\n",
      "\n",
      "test loss:  0.000518110146923209\n",
      "accuracy:  98.84\n",
      "precision:  0.9884108243235536\n",
      "recall:  0.9882427870655677\n",
      "f1:  0.9883082111400767\n",
      "\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:20<00:00, 45.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Average loss: 0.008326\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 9895/10000 (99%), Precision: 0.99, Recall: 0.99, F1 Score: 0.99\n",
      "\n",
      "0.00010737418240000007\n",
      "\n",
      "test loss:  0.0004997915303645641\n",
      "accuracy:  98.95\n",
      "precision:  0.9895057764742223\n",
      "recall:  0.9893265931720059\n",
      "f1:  0.9894043251317456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = convolution_kan.CNN_KAN()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 10\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch, criterion)\n",
    "        test_loss, accuracy, precision, recall, f1 = test(model, device, test_loader, criterion)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "        scheduler.step()\n",
    "        print('')\n",
    "        print(\"lr: \", optimizer.param_groups[0]['lr'])\n",
    "        print(\"test loss: \", test_loss)\n",
    "        print(\"accuracy: \", accuracy)\n",
    "        print(\"precision: \", precision)\n",
    "        print(\"recall: \", recall)\n",
    "        print(\"f1: \", f1)\n",
    "        print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
